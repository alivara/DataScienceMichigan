{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a7460f3e3c42534125a0802936889559",
     "grade": false,
     "grade_id": "cell-fa48e7f1b94baa5b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 1\n",
    "For this assignment you are welcomed to use other regex resources such a regex \"cheat sheets\" you find on the web.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d17f561e3c6c08092810b982d085f5be",
     "grade": false,
     "grade_id": "cell-d4da7eb9acee2a6d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Before start working on the problems, here is a small example to help you understand how to write your own answers. In short, the solution should be written within the function body given, and the final result should be returned. Then the autograder will try to call the function and validate your returned result accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7eeb5e7d0f0e0137caed9f3b5cb925b1",
     "grade": false,
     "grade_id": "cell-4a96535829224b3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def example_word_count():\n",
    "    # This example question requires counting words in the example_string below.\n",
    "    example_string = \"Amy is 5 years old\"\n",
    "    \n",
    "    # YOUR CODE HERE.\n",
    "    # You should write your solution here, and return your result, you can comment out or delete the\n",
    "    # NotImplementedError below.\n",
    "    result = example_string.split(\" \")\n",
    "    return len(result)\n",
    "\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Find a list of all of the names in the following string using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "29bc8c161c0e246c1e3ef4820cc164f7",
     "grade": false,
     "grade_id": "names",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def names():\n",
    "    simple_string =\"\"\"Amy is 5 years old, and her sister Mary is 2 years old.Ruth and Peter, their parents, have 3 kids.\"\"\"\n",
    "    pattern='[A-Z][a-z]*'\n",
    "    result=re.findall(pattern,simple_string)\n",
    "    return result\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed5c09ac57f7d98130d5abc557f6d6c4",
     "grade": true,
     "grade_id": "correct_names",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(names()) == 4, \"There are four names in the simple_string\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77b3d100c47e9e41d98f82dfeb7eba9c",
     "grade": false,
     "grade_id": "cell-ed64e3464ddd7ba7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part B\n",
    "\n",
    "The dataset file in [assets/grades.txt](assets/grades.txt) contains a line separated list of people with their grade in \n",
    "a class. Create a regex to generate a list of just those students who received a B in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e977a1df674e9fa684e6d172aec92824",
     "grade": false,
     "grade_id": "grades",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def grades():\n",
    "    with open (\"assets/grades.txt\", \"r\") as file:\n",
    "        grades = file.read()\n",
    "    result = re.findall('(.*): B',grades)\n",
    "    return result\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0bcc452d60fc45259e58d3116d25477",
     "grade": true,
     "grade_id": "correct_grades",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'assets/grades.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m5/gjzyxs7575z5k5j0xxzrvzlc0000gn/T/ipykernel_9841/1885468736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrades\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/m5/gjzyxs7575z5k5j0xxzrvzlc0000gn/T/ipykernel_9841/4293573819.py\u001b[0m in \u001b[0;36mgrades\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrades\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"assets/grades.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mgrades\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(.*): B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrades\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/grades.txt'"
     ]
    }
   ],
   "source": [
    "assert len(grades()) == 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "36e3e2a3a3e29fa7b823d22476392320",
     "grade": false,
     "grade_id": "cell-e253518e37d33f0c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part C\n",
    "\n",
    "Consider the standard web log file in [assets/logdata.txt](assets/logdata.txt). This file records the access a user makes when visiting a web page (like this one!). Each line of the log has the following items:\n",
    "* a host (e.g., '146.204.224.152') \n",
    "* a user_name (e.g., 'feest6811' **note: sometimes the user name is missing! In this case, use '-' as the value for the username.**)\n",
    "* the time a request was made (e.g., '21/Jun/2019:15:45:24 -0700')\n",
    "* the post request type (e.g., 'POST /incentivize HTTP/1.1' **note: not everything is a POST!**)\n",
    "\n",
    "Your task is to convert this into a list of dictionaries, where each dictionary looks like the following:\n",
    "```\n",
    "example_dict = {\"host\":\"146.204.224.152\", \n",
    "                \"user_name\":\"feest6811\", \n",
    "                \"time\":\"21/Jun/2019:15:45:24 -0700\",\n",
    "                \"request\":\"POST /incentivize HTTP/1.1\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c04017e59e48b2f4c77bf425ed84b356",
     "grade": false,
     "grade_id": "logs",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def logs():\n",
    "    result=list()\n",
    "    with open(\"assets/logdata.txt\", \"r\") as file:\n",
    "        logdata = file.read()\n",
    "        pattern=\"\"\"\n",
    "        (?P<host>[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)\n",
    "        (\\ - \\ )\n",
    "        (?P<user_name>(\\w*)(\\S))\n",
    "        (\\  \\S)\n",
    "        (?P<time>\\d+\\S\\w*\\S\\d+\\S\\d+\\S\\d+\\S\\d+\\s\\S\\d+)\n",
    "        (\\S\\s\\S)\n",
    "        (?P<request>\\w*\\s\\S*\\s\\w*\\S\\d.\\d*)\n",
    "        \"\"\"\n",
    "    for item in re.finditer(pattern,logdata,re.VERBOSE):\n",
    "        result.append(item.groupdict())\n",
    "    return result\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1fd5f2cca190d37c667fb189352540d3",
     "grade": true,
     "grade_id": "cell-correct_logs",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'assets/logdata.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m5/gjzyxs7575z5k5j0xxzrvzlc0000gn/T/ipykernel_9841/3220867527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m979\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m one_item={'host': '146.204.224.152',\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m'user_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'feest6811'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m'time'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'21/Jun/2019:15:45:24 -0700'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m5/gjzyxs7575z5k5j0xxzrvzlc0000gn/T/ipykernel_9841/2475100478.py\u001b[0m in \u001b[0;36mlogs\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"assets/logdata.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlogdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         pattern=\"\"\"\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/logdata.txt'"
     ]
    }
   ],
   "source": [
    "assert len(logs()) == 979\n",
    "\n",
    "one_item={'host': '146.204.224.152',\n",
    "  'user_name': 'feest6811',\n",
    "  'time': '21/Jun/2019:15:45:24 -0700',\n",
    "  'request': 'POST /incentivize HTTP/1.1'}\n",
    "assert one_item in logs(), \"Sorry, this item should be in the log results, check your formating\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "For this assignment you'll be looking at 2017 data on immunizations from the CDC. Your datafile for this assignment is in [assets/NISPUF17.csv](assets/NISPUF17.csv). A data users guide for this, which you'll need to map the variables in the data to the questions being asked, is available at [assets/NIS-PUF17-DUG.pdf](assets/NIS-PUF17-DUG.pdf). **Note: you may have to go to your Jupyter tree (click on the Coursera image) and navigate to the assignment 2 assets folder to see this PDF file).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Write a function called `proportion_of_education` which returns the proportion of children in the dataset who had a mother with the education levels equal to less than high school (<12), high school (12), more than high school but not a college graduate (>12) and college degree.\n",
    "\n",
    "*This function should return a dictionary in the form of (use the correct numbers, do not round numbers):* \n",
    "```\n",
    "    {\"less than high school\":0.2,\n",
    "    \"high school\":0.4,\n",
    "    \"more than high school but not college\":0.2,\n",
    "    \"college\":0.2}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_of_education():\n",
    "    # your code goes here\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('assets/NISPUF17.csv', index_col=0)\n",
    "    EDUC1 = df['EDUC1'].value_counts(normalize=True)\n",
    "    EDUC1_dict = {\"less than high school\": EDUC1.iloc[3],\n",
    "                   \"high school\": EDUC1.iloc[2],\n",
    "                   \"more than high school but not college\": EDUC1.iloc[1],\n",
    "                   \"college\": EDUC1.iloc[0]}\n",
    "    return EDUC1_dict\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Let's explore the relationship between being fed breastmilk as a child and getting a seasonal influenza vaccine from a healthcare provider. Return a tuple of the average number of influenza vaccines for those children we know received breastmilk as a child and those who know did not.\n",
    "\n",
    "*This function should return a tuple in the form (use the correct numbers:*\n",
    "```\n",
    "(2.5, 0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_influenza_doses():\n",
    "    # YOUR CODE HERE\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('assets/NISPUF17.csv')\n",
    "    df1 = df[df[\"CBF_01\"] == 1]\n",
    "    df2 = df[df[\"CBF_01\"] == 2]\n",
    "    \n",
    "    return (df1[\"P_NUMFLU\"].mean(),df2[\"P_NUMFLU\"].mean())\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "It would be interesting to see if there is any evidence of a link between vaccine effectiveness and sex of the child. Calculate the ratio of the number of children who contracted chickenpox but were vaccinated against it (at least one varicella dose) versus those who were vaccinated but did not contract chicken pox. Return results by sex. \n",
    "\n",
    "*This function should return a dictionary in the form of (use the correct numbers):* \n",
    "```\n",
    "    {\"male\":0.2,\n",
    "    \"female\":0.4}\n",
    "```\n",
    "\n",
    "Note: To aid in verification, the `chickenpox_by_sex()['female']` value the autograder is looking for starts with the digits `0.0077`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chickenpox_by_sex():\n",
    "    # YOUR CODE HERE\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('assets/NISPUF17.csv')\n",
    "    hdyp = df[(df[\"HAD_CPOX\"] ==1) & (df[\"P_NUMVRC\"] >= 1)]\n",
    "    hdnp = df[(df[\"HAD_CPOX\"] ==2) & (df[\"P_NUMVRC\"] >= 1)]\n",
    "\n",
    "    m_ratio_p = len(hdyp[hdyp[\"SEX\"] == 1])/len(hdnp[hdnp[\"SEX\"] == 1])\n",
    "    f_ratio_p = len(hdyp[hdyp[\"SEX\"] == 2])/len(hdnp[hdnp[\"SEX\"] == 2])\n",
    "\n",
    "    \n",
    "    result_dic ={\"male\": m_ratio_p,\n",
    "                \"female\": f_ratio_p}\n",
    "    return result_dic\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "A correlation is a statistical relationship between two variables. If we wanted to know if vaccines work, we might look at the correlation between the use of the vaccine and whether it results in prevention of the infection or disease [1]. In this question, you are to see if there is a correlation between having had the chicken pox and the number of chickenpox vaccine doses given (varicella).\n",
    "\n",
    "Some notes on interpreting the answer. The `had_chickenpox_column` is either `1` (for yes) or `2` (for no), and the `num_chickenpox_vaccine_column` is the number of doses a child has been given of the varicella vaccine. A positive correlation (e.g., `corr > 0`) means that an increase in `had_chickenpox_column` (which means more no’s) would also increase the values of `num_chickenpox_vaccine_column` (which means more doses of vaccine). If there is a negative correlation (e.g., `corr < 0`), it indicates that having had chickenpox is related to an increase in the number of vaccine doses.\n",
    "\n",
    "Also, `pval` is the probability that we observe a correlation between `had_chickenpox_column` and `num_chickenpox_vaccine_column` which is greater than or equal to a particular value occurred by chance. A small `pval` means that the observed correlation is highly unlikely to occur by chance. In this case, `pval` should be very small (will end in `e-18` indicating a very small number).\n",
    "\n",
    "[1] This isn’t really the full picture, since we are not looking at when the dose was given. It’s possible that children had chickenpox and then their parents went to get them the vaccine. Does this dataset have the data we would need to investigate the timing of the dose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_chickenpox():\n",
    "    import scipy.stats as stats\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_csv('assets/NISPUF17.csv', index_col=0)\n",
    "    df = df[df[\"HAD_CPOX\"] <= 2]\n",
    "    df = df[~df[\"P_NUMVRC\"].isna() & ~df[\"HAD_CPOX\"].isna()]\n",
    "\n",
    "    # here is some stub code to actually run the correlation\n",
    "    corr, pval=stats.pearsonr(df[\"HAD_CPOX\"], df[\"P_NUMVRC\"])\n",
    "    \n",
    "    # just return the correlation\n",
    "    return corr\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "All questions are weighted the same in this assignment. This assignment requires more individual learning then the last one did - you are encouraged to check out the [pandas documentation](http://pandas.pydata.org/pandas-docs/stable/) to find functions or methods you might not have used yet, or ask questions on [Stack Overflow](http://stackoverflow.com/) and tag them as pandas and python related. All questions are worth the same number of points except question 1 which is worth 17% of the assignment grade.\n",
    "\n",
    "**Note**: Questions 3-13 rely on your question 1 answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filter all warnings. If you would like to see the warnings, please comment the two lines below.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Load the energy data from the file `assets/Energy Indicators.xls`, which is a list of indicators of [energy supply and renewable electricity production](assets/Energy%20Indicators.xls) from the [United Nations](http://unstats.un.org/unsd/environment/excel_file_tables/2013/Energy%20Indicators.xls) for the year 2013, and should be put into a DataFrame with the variable name of **Energy**.\n",
    "\n",
    "Keep in mind that this is an Excel file, and not a comma separated values file. Also, make sure to exclude the footer and header information from the datafile. The first two columns are unneccessary, so you should get rid of them, and you should change the column labels so that the columns are:\n",
    "\n",
    "`['Country', 'Energy Supply', 'Energy Supply per Capita', '% Renewable]`\n",
    "\n",
    "Convert `Energy Supply` to gigajoules (**Note: there are 1,000,000 gigajoules in a petajoule**). For all countries which have missing data (e.g. data with \"...\") make sure this is reflected as `np.NaN` values.\n",
    "\n",
    "Rename the following list of countries (for use in later questions):\n",
    "\n",
    "```\"Republic of Korea\": \"South Korea\",\n",
    "\"United States of America\": \"United States\",\n",
    "\"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "\"China, Hong Kong Special Administrative Region\": \"Hong Kong\"```\n",
    "\n",
    "There are also several countries with numbers and/or parenthesis in their name. Be sure to remove these, e.g. `'Bolivia (Plurinational State of)'` should be `'Bolivia'`.  `'Switzerland17'` should be `'Switzerland'`.\n",
    "\n",
    "Next, load the GDP data from the file `assets/world_bank.csv`, which is a csv containing countries' GDP from 1960 to 2015 from [World Bank](http://data.worldbank.org/indicator/NY.GDP.MKTP.CD). Call this DataFrame **GDP**. \n",
    "\n",
    "Make sure to skip the header, and rename the following list of countries:\n",
    "\n",
    "```\"Korea, Rep.\": \"South Korea\", \n",
    "\"Iran, Islamic Rep.\": \"Iran\",\n",
    "\"Hong Kong SAR, China\": \"Hong Kong\"```\n",
    "\n",
    "Finally, load the [Sciamgo Journal and Country Rank data for Energy Engineering and Power Technology](http://www.scimagojr.com/countryrank.php?category=2102) from the file `assets/scimagojr-3.xlsx`, which ranks countries based on their journal contributions in the aforementioned area. Call this DataFrame **ScimEn**.\n",
    "\n",
    "Join the three datasets: GDP, Energy, and ScimEn into a new dataset (using the intersection of country names). Use only the last 10 years (2006-2015) of GDP data and only the top 15 countries by Scimagojr 'Rank' (Rank 1 through 15). \n",
    "\n",
    "The index of this DataFrame should be the name of the country, and the columns should be ['Rank', 'Documents', 'Citable documents', 'Citations', 'Self-citations',\n",
    "       'Citations per document', 'H index', 'Energy Supply',\n",
    "       'Energy Supply per Capita', '% Renewable', '2006', '2007', '2008',\n",
    "       '2009', '2010', '2011', '2012', '2013', '2014', '2015'].\n",
    "\n",
    "*This function should return a DataFrame with 20 columns and 15 entries, and the rows of the DataFrame should be sorted by \"Rank\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Energy dataset\n",
    "    Energy = pd.read_excel('assets/Energy Indicators.xls',skiprows=0,header=17,skipfooter=38)\n",
    "    del Energy['Unnamed: 0']\n",
    "    del Energy['Unnamed: 2']\n",
    "    Energy.rename(columns={'Petajoules':'Energy Supply',\n",
    "                               'Gigajoules':'Energy Supply per Capita'\n",
    "                               ,'%':'% Renewable',\n",
    "                                'Unnamed: 1':'Country'},inplace=True)\n",
    "    # change name Energy\n",
    "    Energy = Energy.replace({\"...\": np.NaN,\"Republic of Korea\": \"South Korea\",\n",
    "                             \"United States of America\": \"United States\",\n",
    "                            \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "                            \"China, Hong Kong Special Administrative Region\": \"Hong Kong\"})\n",
    "    # conver Energy Supply to Gigajoules\n",
    "    Energy['Energy Supply'] *= 1000000\n",
    "    # delete (...)\n",
    "    Energy[\"Country\"] = Energy[\"Country\"].str.replace(' [()].*[)]', '',regex=True)\n",
    "    \n",
    "    # GDP dataset\n",
    "    GDP = pd.read_csv('assets/world_bank.csv',header=4)\n",
    "    # change name GDP\n",
    "    GDP = GDP.replace({\"Korea, Rep.\": \"South Korea\",\n",
    "                   \"Iran, Islamic Rep.\": \"Iran\",\n",
    "                   \"Hong Kong SAR, China\": \"Hong Kong\"})\n",
    "    GDP = GDP.rename(columns={\"Country Name\":\"Country\"})\n",
    "    \n",
    "    # ScimEn dataset\n",
    "    ScimEn = pd.read_excel('assets/scimagojr-3.xlsx')\n",
    "    \n",
    "    # setting index to Country\n",
    "    Energy.set_index(['Country'],inplace=True) \n",
    "    ScimEn.set_index(['Country'],inplace=True)\n",
    "    GDP.set_index(['Country'],inplace=True)\n",
    "    \n",
    "    # define ScimEn dataset for merge\n",
    "    df1 = ScimEn[['Rank', 'Documents', 'Citable documents', 'Citations',\n",
    "              'Self-citations', 'Citations per document', 'H index']]\n",
    "    df1 = df1[df1['Rank'] <= 15]\n",
    "    \n",
    "    # define Energy dataset for merge\n",
    "    df2 = Energy[['Energy Supply', 'Energy Supply per Capita', '% Renewable']]\n",
    "    \n",
    "    # define GDP dataset for merge\n",
    "    df3 = GDP[['2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']]\n",
    "    \n",
    "    # define final dataset \n",
    "    df_final = df1.merge(df2,left_index=True,\n",
    "                     right_index=True,how='inner').merge(df3,left_index=True,\n",
    "                                                         right_index=True,how='inner')\n",
    "    return df_final\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "The previous question joined three datasets then reduced this to just the top 15 entries. When you joined the datasets, but before you reduced this to the top 15 items, how many entries did you lose?\n",
    "\n",
    "*This function should return a single number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<svg width=\"800\" height=\"300\">\n",
    "  <circle cx=\"150\" cy=\"180\" r=\"80\" fill-opacity=\"0.2\" stroke=\"black\" stroke-width=\"2\" fill=\"blue\" />\n",
    "  <circle cx=\"200\" cy=\"100\" r=\"80\" fill-opacity=\"0.2\" stroke=\"black\" stroke-width=\"2\" fill=\"red\" />\n",
    "  <circle cx=\"100\" cy=\"100\" r=\"80\" fill-opacity=\"0.2\" stroke=\"black\" stroke-width=\"2\" fill=\"green\" />\n",
    "  <line x1=\"150\" y1=\"125\" x2=\"300\" y2=\"150\" stroke=\"black\" stroke-width=\"2\" fill=\"black\" stroke-dasharray=\"5,3\"/>\n",
    "  <text x=\"300\" y=\"165\" font-family=\"Verdana\" font-size=\"35\">Everything but this!</text>\n",
    "</svg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    # YOUR CODE HERE\n",
    "    # Energy dataset\n",
    "    Energy = pd.read_excel('assets/Energy Indicators.xls',skiprows=0,header=17,skipfooter=38)\n",
    "    del Energy['Unnamed: 0']\n",
    "    del Energy['Unnamed: 2']\n",
    "    Energy.rename(columns={'Petajoules':'Energy Supply',\n",
    "                               'Gigajoules':'Energy Supply per Capita'\n",
    "                               ,'%':'% Renewable',\n",
    "                                'Unnamed: 1':'Country'},inplace=True)\n",
    "    # change name Energy\n",
    "    Energy = Energy.replace({\"...\": np.NaN,\"Republic of Korea\": \"South Korea\",\n",
    "                             \"United States of America\": \"United States\",\n",
    "                            \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "                            \"China, Hong Kong Special Administrative Region\": \"Hong Kong\"})\n",
    "    # conver Energy Supply to Gigajoules\n",
    "    Energy['Energy Supply'] *= 1000000\n",
    "    # delete (...)\n",
    "    Energy[\"Country\"] = Energy[\"Country\"].str.replace(' [()].*[)]', '',regex=True)\n",
    "    \n",
    "    # GDP dataset\n",
    "    GDP = pd.read_csv('assets/world_bank.csv',header=4)\n",
    "    # change name GDP\n",
    "    GDP = GDP.replace({\"Korea, Rep.\": \"South Korea\",\n",
    "                   \"Iran, Islamic Rep.\": \"Iran\",\n",
    "                   \"Hong Kong SAR, China\": \"Hong Kong\"})\n",
    "    GDP = GDP.rename(columns={\"Country Name\":\"Country\"})\n",
    "    \n",
    "    # ScimEn dataset\n",
    "    ScimEn = pd.read_excel('assets/scimagojr-3.xlsx')\n",
    "    \n",
    "    # setting index to Country\n",
    "    Energy.set_index(['Country'],inplace=True) \n",
    "    ScimEn.set_index(['Country'],inplace=True)\n",
    "    GDP.set_index(['Country'],inplace=True)\n",
    "    \n",
    "    # define ScimEn dataset for merge\n",
    "    df1 = ScimEn[['Rank', 'Documents', 'Citable documents', 'Citations',\n",
    "              'Self-citations', 'Citations per document', 'H index']]\n",
    "    df1 = df1[df1['Rank'] <= 15]\n",
    "    \n",
    "    # define Energy dataset for merge\n",
    "    df2 = Energy[['Energy Supply', 'Energy Supply per Capita', '% Renewable']]\n",
    "    \n",
    "    # define GDP dataset for merge\n",
    "    df3 = GDP[['2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']]\n",
    "    \n",
    "    # define final dataset \n",
    "#     df_final_1 = df1.merge(df2,left_index=True,\n",
    "#                      right_index=True,how='outer').merge(df3,left_index=True,\n",
    "#                                                          right_index=True,how='outer')\n",
    "#     df_final_2 = df1.merge(df2,left_index=True,\n",
    "#                      right_index=True,how='inner').merge(df3,left_index=True,\n",
    "#                                                          right_index=True,how='inner')\n",
    "#     delta = df_final_1.shape[0] - df_final_2.shape[0]\n",
    "#     return delta\n",
    "\n",
    "    union = pd.merge(pd.merge(Energy, GDP, on='Country', how='outer'), ScimEn, on='Country', how='outer')\n",
    "    intersect = pd.merge(pd.merge(Energy, GDP, on='Country'), ScimEn, on='Country')\n",
    "    return len(union)-len(intersect)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What are the top 15 countries for average GDP over the last 10 years?\n",
    "\n",
    "*This function should return a Series named `avgGDP` with 15 countries and their average GDP sorted in descending order.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    # YOUR CODE HERE\n",
    "    # if there was no other functions\n",
    "    \n",
    "#     GDP = pd.read_csv('assets/world_bank.csv',header=4)\n",
    "#     GDP = GDP.replace({\"Korea, Rep.\": \"South Korea\",\n",
    "#                        \"Iran, Islamic Rep.\": \"Iran\",\n",
    "#                        \"Hong Kong SAR, China\": \"Hong Kong\"})\n",
    "#     GDP = GDP.rename(columns={\"Country Name\":\"Country\"})\n",
    "#     df_avg = GDP[['Country','2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']]\n",
    "#     df_avg = df_avg.set_index('Country')\n",
    "#     df_avg['mean'] = df_avg.mean(axis=1)\n",
    "#     df_avg = df_avg[['mean']].squeeze()\n",
    "    \n",
    "#     return df_avg.nlargest(15)\n",
    "\n",
    "    # by using other functions\n",
    "    \n",
    "    Top15 = answer_one()\n",
    "    rows = ['2006', '2007', '2008','2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "    Top15[\"avgGDP\"] = Top15[rows].mean(axis=1)\n",
    "    return Top15.sort_values(\"avgGDP\",ascending=False)[\"avgGDP\"]\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "By how much had the GDP changed over the 10 year span for the country with the 6th largest average GDP?\n",
    "\n",
    "*This function should return a single number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    # YOUR CODE HERE\n",
    "    # if there was no other functions \n",
    "    \n",
    "#     GDP = pd.read_csv('assets/world_bank.csv',header=4)\n",
    "#     GDP = GDP.replace({\"Korea, Rep.\": \"South Korea\",\n",
    "#                        \"Iran, Islamic Rep.\": \"Iran\",\n",
    "#                        \"Hong Kong SAR, China\": \"Hong Kong\"})\n",
    "#     GDP = GDP.rename(columns={\"Country Name\":\"Country\"})\n",
    "#     df_avg = GDP[['Country','2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']]\n",
    "#     df_avg = df_avg.set_index('Country')\n",
    "#     df_avg['mean'] = df_avg.mean(axis=1)\n",
    "#     se_avg = df_avg[['mean']].squeeze()\n",
    "#     se_avg = se_avg.sort_values(ascending=False)\n",
    "#     country_1 = se_avg_sorted[se_avg_sorted == se_avg_sorted[5]].index[0]\n",
    "#     return (df_avg.loc[country_1][0] - df_avg.loc[country_1][-2])\n",
    "\n",
    "    # by using other functions\n",
    "    Top15 = answer_one()\n",
    "    Top15[\"AvgGDP\"] = answer_three()\n",
    "    Top15.sort_values(\"AvgGDP\", ascending=False, inplace=True)\n",
    "    final = Top15.iloc[5]['2015']\n",
    "    initial = Top15.iloc[5]['2006']\n",
    "    return abs(final - initial)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "What is the mean energy supply per capita?\n",
    "\n",
    "*This function should return a single number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    # YOUR CODE HERE\n",
    "    # if there was no other functions \n",
    "    \n",
    "#     Energy = pd.read_excel('assets/Energy Indicators.xls',skiprows=0,header=17,skipfooter=38)\n",
    "#     del Energy['Unnamed: 0']\n",
    "#     del Energy['Unnamed: 2']\n",
    "#     Energy.rename(columns={'Petajoules':'Energy Supply',\n",
    "#                                'Gigajoules':'Energy Supply per Capita'\n",
    "#                                ,'%':'% Renewable',\n",
    "#                                 'Unnamed: 1':'Country'},inplace=True)\n",
    "#     # change name Energy\n",
    "#     Energy = Energy.replace({\"...\": np.NaN})\n",
    "#     # conver Energy Supply to Gigajoules\n",
    "    \n",
    "#     Energy['Energy Supply'] *= 1000000\n",
    "#     return Energy['Energy Supply'].mean()\n",
    "\n",
    "    # by using other functions\n",
    "    Top15 = answer_one()\n",
    "    return Top15['Energy Supply per Capita'].mean()\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "What country has the maximum % Renewable and what is the percentage?\n",
    "\n",
    "*This function should return a tuple with the name of the country and the percentage.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    # YOUR CODE HERE\n",
    "    # if there was no other functions \n",
    "    \n",
    "#     Energy = pd.read_excel('assets/Energy Indicators.xls',skiprows=0,header=17,skipfooter=38)\n",
    "#     del Energy['Unnamed: 0']\n",
    "#     del Energy['Unnamed: 2']\n",
    "#     Energy.rename(columns={'Petajoules':'Energy Supply',\n",
    "#                                'Gigajoules':'Energy Supply per Capita'\n",
    "#                                ,'%':'% Renewable',\n",
    "#                                 'Unnamed: 1':'Country'},inplace=True)\n",
    "#     # change name Energy\n",
    "#     Energy = Energy.replace({\"...\": np.NaN})\n",
    "\n",
    "\n",
    "#     max_ren = Energy['% Renewable'].max()\n",
    "#     max_country = Energy['Country'][Energy['% Renewable'][Energy['% Renewable'] == Energy['% Renewable'].max()].index[0]]\n",
    "\n",
    "#     return (max_country,max_ren)\n",
    "\n",
    "    # by using other functions\n",
    "    \n",
    "    Top15 = answer_one()\n",
    "    return Top15['% Renewable'].argmax(), Top15['% Renewable'].max()\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Create a new column that is the ratio of Self-Citations to Total Citations. \n",
    "What is the maximum value for this new column, and what country has the highest ratio?\n",
    "\n",
    "*This function should return a tuple with the name of the country and the ratio.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    # YOUR CODE HERE\n",
    "    # if there was no other functions \n",
    "    \n",
    "#     ScimEn = pd.read_excel('assets/scimagojr-3.xlsx')\n",
    "\n",
    "#     ScimEn['RST'] = (ScimEn['Self-citations']/ScimEn['Citable documents'] \n",
    "#                                               \n",
    "\n",
    "#     max_ratio = ScimEn['RST'].max()\n",
    "#     max_country = ScimEn['Country'][ScimEn['RST'][ScimEn['RST']== ScimEn['RST'].max()].index[0]]\n",
    "\n",
    "#     return (max_country,max_ratio)\n",
    "\n",
    "    # by using other functions\n",
    "    \n",
    "    Top15 = answer_one()\n",
    "    Top15[\"Ratio\"] = Top15[\"Self-citations\"] / Top15['Citations']\n",
    "    return (Top15[\"Ratio\"].argmax(), Top15[\"Ratio\"].max())\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Create a column that estimates the population using Energy Supply and Energy Supply per capita. \n",
    "What is the third most populous country according to this estimate?\n",
    "\n",
    "*This function should return the name of the country*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    # YOUR CODE HERE\n",
    "    # if there was no other functions\n",
    "#     ScimEn = pd.read_excel('assets/scimagojr-3.xlsx')\n",
    "#     Energy = pd.read_excel('assets/Energy Indicators.xls',skiprows=0,header=17,skipfooter=38)\n",
    "#     del Energy['Unnamed: 0']\n",
    "#     del Energy['Unnamed: 2']\n",
    "#     Energy.rename(columns={'Petajoules':'Energy Supply',\n",
    "#                                'Gigajoules':'Energy Supply per Capita'\n",
    "#                                ,'%':'% Renewable',\n",
    "#                                 'Unnamed: 1':'Country'},inplace=True)\n",
    "#     # change name Energy\n",
    "#     Energy = Energy.replace({\"...\": np.NaN})\n",
    "#     # conver Energy Supply to Gigajoules\n",
    "#     Energy['Energy Supply'] *= 1000000\n",
    "#     Energy[\"Population\"] =  Energy['Energy Supply'] / Energy['Energy Supply per Capita']\n",
    "#     final = Energy.sort_values(\"Population\", ascending=False)\n",
    "#     return final.iloc[2].name\n",
    "\n",
    "    # by using other functions\n",
    "    \n",
    "    Top15 = answer_one()\n",
    "    Top15[\"Population\"] = Top15['Energy Supply'] / Top15['Energy Supply per Capita']\n",
    "    final = Top15.sort_values(\"Population\", ascending=False)\n",
    "    return final.iloc[2].name\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Create a column that estimates the number of citable documents per person. \n",
    "What is the correlation between the number of citable documents per capita and the energy supply per capita? Use the `.corr()` method, (Pearson's correlation).\n",
    "\n",
    "*This function should return a single number.*\n",
    "\n",
    "*(Optional: Use the built-in function `plot9()` to visualize the relationship between Energy Supply per Capita vs. Citable docs per Capita)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_nine():\n",
    "    # YOUR CODE HERE\n",
    "    # if there was no other functions\n",
    "    \n",
    "#     Energy = pd.read_excel('assets/Energy Indicators.xls',skiprows=0,header=17,skipfooter=38)\n",
    "#     del Energy['Unnamed: 0']\n",
    "#     del Energy['Unnamed: 2']\n",
    "#     Energy.rename(columns={'Petajoules':'Energy Supply',\n",
    "#                                'Gigajoules':'Energy Supply per Capita'\n",
    "#                                ,'%':'% Renewable',\n",
    "#                                 'Unnamed: 1':'Country'},inplace=True)\n",
    "#     # change name Energy\n",
    "#     Energy = Energy.replace({\"...\": np.NaN})\n",
    "#     # conver Energy Supply to Gigajoules\n",
    "#     Energy['Energy Supply'] *= 1000000\n",
    "#     Energy[\"Population\"] =  Energy['Energy Supply'] / Energy['Energy Supply per Capita']\n",
    "#     ScimEn[\"Citable docs per Capita\"] = ScimEn[\"Citable documents\"] / Energy[\"Population\"]\n",
    "#     ScimEn[\"Citable docs per Capita\"].corr(Energy['Energy Supply per Capita']) \n",
    "\n",
    "    # by using other functions\n",
    "    \n",
    "    Top15 = answer_one()\n",
    "    Top15[\"Population\"] = Top15['Energy Supply'] / Top15['Energy Supply per Capita']\n",
    "    Top15[\"Citable docs per Capita\"] = Top15[\"Citable documents\"] / Top15[\"Population\"]\n",
    "    return Top15[\"Citable docs per Capita\"].corr(Top15['Energy Supply per Capita'])\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot9():\n",
    "    import matplotlib as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    Top15 = answer_one()\n",
    "    Top15['PopEst'] = Top15['Energy Supply'] / Top15['Energy Supply per Capita']\n",
    "    Top15['Citable docs per Capita'] = Top15['Citable documents'] / Top15['PopEst']\n",
    "    Top15.plot(x='Citable docs per Capita', y='Energy Supply per Capita', kind='scatter', xlim=[0, 0.0006])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Create a new column with a 1 if the country's % Renewable value is at or above the median for all countries in the top 15, and a 0 if the country's % Renewable value is below the median.\n",
    "\n",
    "*This function should return a series named `HighRenew` whose index is the country name sorted in ascending order of rank.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_ten():\n",
    "    # YOUR CODE HERE\n",
    "    Top15 = answer_one()\n",
    "    reference = Top15[\"% Renewable\"].median(axis=0)\n",
    "    Top15[\"HighRenew\"] = Top15.apply(lambda x: 1 if x[\"% Renewable\"] >= reference else 0, axis=1)\n",
    "    Top15.sort_values(by='Rank', inplace=True)\n",
    "    return Top15[\"HighRenew\"] \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Use the following dictionary to group the Countries by Continent, then create a DataFrame that displays the sample size (the number of countries in each continent bin), and the sum, mean, and std deviation for the estimated population of each country.\n",
    "\n",
    "```python\n",
    "ContinentDict  = {'China':'Asia', \n",
    "                  'United States':'North America', \n",
    "                  'Japan':'Asia', \n",
    "                  'United Kingdom':'Europe', \n",
    "                  'Russian Federation':'Europe', \n",
    "                  'Canada':'North America', \n",
    "                  'Germany':'Europe', \n",
    "                  'India':'Asia',\n",
    "                  'France':'Europe', \n",
    "                  'South Korea':'Asia', \n",
    "                  'Italy':'Europe', \n",
    "                  'Spain':'Europe', \n",
    "                  'Iran':'Asia',\n",
    "                  'Australia':'Australia', \n",
    "                  'Brazil':'South America'}\n",
    "```\n",
    "\n",
    "*This function should return a DataFrame with index named Continent `['Asia', 'Australia', 'Europe', 'North America', 'South America']` and columns `['size', 'sum', 'mean', 'std']`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eleven():\n",
    "    # YOUR CODE HERE\n",
    "    ContinentDict  = {'China':'Asia', \n",
    "                  'United States':'North America', \n",
    "                  'Japan':'Asia', \n",
    "                  'United Kingdom':'Europe', \n",
    "                  'Russian Federation':'Europe', \n",
    "                  'Canada':'North America', \n",
    "                  'Germany':'Europe', \n",
    "                  'India':'Asia',\n",
    "                  'France':'Europe', \n",
    "                  'South Korea':'Asia', \n",
    "                  'Italy':'Europe', \n",
    "                  'Spain':'Europe', \n",
    "                  'Iran':'Asia',\n",
    "                  'Australia':'Australia', \n",
    "                  'Brazil':'South America'}\n",
    "    \n",
    "    Top15 = answer_one()\n",
    "    final_c = pd.DataFrame(columns=['size', 'sum', 'mean', 'std'])\n",
    "    Top15['Estimate Population'] = Top15['Energy Supply'] / Top15['Energy Supply per Capita']\n",
    "    for group, contient in Top15.groupby(ContinentDict):\n",
    "        final_c.loc[group] = [len(contient), contient['Estimate Population'].sum(),\n",
    "                            contient['Estimate Population'].mean(),\n",
    "                            contient['Estimate Population'].std()]\n",
    "    return final_c\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "Cut % Renewable into 5 bins. Group Top15 by the Continent, as well as these new % Renewable bins. How many countries are in each of these groups?\n",
    "\n",
    "*This function should return a Series with a MultiIndex of `Continent`, then the bins for `% Renewable`. Do not include groups with no countries.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_twelve():\n",
    "    Top15 = answer_one()\n",
    "    ContinentDict  = {'China':'Asia',\n",
    "                      'United States':'North America', \n",
    "                      'Japan':'Asia', \n",
    "                      'United Kingdom':'Europe', \n",
    "                      'Russian Federation':'Europe', \n",
    "                      'Canada':'North America', \n",
    "                      'Germany':'Europe', \n",
    "                      'India':'Asia',\n",
    "                      'France':'Europe', \n",
    "                      'South Korea':'Asia', \n",
    "                      'Italy':'Europe', \n",
    "                      'Spain':'Europe', \n",
    "                      'Iran':'Asia',\n",
    "                      'Australia':'Australia', \n",
    "                      'Brazil':'South America'\n",
    "                     }\n",
    "    \n",
    "    Top15['Continent'] = Top15.index.map(lambda c: ContinentDict[c])\n",
    "    \n",
    "    # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "    Top15['% Renewable bins'] = pd.cut(Top15['% Renewable'], 5)\n",
    "    \n",
    "    return Top15.groupby(['Continent','% Renewable bins']).size()\n",
    "\n",
    "\n",
    "    #raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "Convert the Population Estimate series to a string with thousands separator (using commas). Use all significant digits (do not round the results).\n",
    "\n",
    "e.g. 12345678.90 -> 12,345,678.90\n",
    "\n",
    "*This function should return a series `PopEst` whose index is the country name and whose values are the population estimate string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_thirteen():\n",
    "    # YOUR CODE HERE\n",
    "    Top15 = answer_one()\n",
    "    Top15[\"Population\"] = Top15['Energy Supply'] / Top15['Energy Supply per Capita']\n",
    "    return Top15['Population'].apply(lambda x: '{0:,}'.format(x))\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional\n",
    "\n",
    "Use the built in function `plot_optional()` to see an example visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optional():\n",
    "    import matplotlib as plt\n",
    "    %matplotlib inline\n",
    "    Top15 = answer_one()\n",
    "    ax = Top15.plot(x='Rank', y='% Renewable', kind='scatter', \n",
    "                    c=['#e41a1c','#377eb8','#e41a1c','#4daf4a','#4daf4a','#377eb8','#4daf4a','#e41a1c',\n",
    "                       '#4daf4a','#e41a1c','#4daf4a','#4daf4a','#e41a1c','#dede00','#ff7f00'], \n",
    "                    xticks=range(1,16), s=6*Top15['2014']/10**10, alpha=.75, figsize=[16,6]);\n",
    "\n",
    "    for i, txt in enumerate(Top15.index):\n",
    "        ax.annotate(txt, [Top15['Rank'][i], Top15['% Renewable'][i]], ha='center')\n",
    "\n",
    "    print(\"This is an example of a visualization that can be created to help understand the data. \\\n",
    "This is a bubble chart showing % Renewable vs. Rank. The size of the bubble corresponds to the countries' \\\n",
    "2014 GDP, and the color corresponds to the continent.\")\n",
    "    \n",
    "plot_optional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "## Description\n",
    "In this assignment you must read in a file of metropolitan regions and associated sports teams from [assets/wikipedia_data.html](assets/wikipedia_data.html) and answer some questions about each metropolitan region. Each of these regions may have one or more teams from the \"Big 4\": NFL (football, in [assets/nfl.csv](assets/nfl.csv)), MLB (baseball, in [assets/mlb.csv](assets/mlb.csv)), NBA (basketball, in [assets/nba.csv](assets/nba.csv) or NHL (hockey, in [assets/nhl.csv](assets/nhl.csv)). Please keep in mind that all questions are from the perspective of the metropolitan region, and that this file is the \"source of authority\" for the location of a given sports team. Thus teams which are commonly known by a different area (e.g. \"Oakland Raiders\") need to be mapped into the metropolitan region given (e.g. San Francisco Bay Area). This will require some human data understanding outside of the data you've been given (e.g. you will have to hand-code some names, and might need to google to find out where teams are)!\n",
    "\n",
    "For each sport I would like you to answer the question: **what is the win/loss ratio's correlation with the population of the city it is in?** Win/Loss ratio refers to the number of wins over the number of wins plus the number of losses. Remember that to calculate the correlation with [`pearsonr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html), so you are going to send in two ordered lists of values, the populations from the wikipedia_data.html file and the win/loss ratio for a given sport in the same order. Average the win/loss ratios for those cities which have multiple teams of a single sport. Each sport is worth an equal amount in this assignment (20%\\*4=80%) of the grade for this assignment. You should only use data **from year 2018** for your analysis -- this is important!\n",
    "\n",
    "## Notes\n",
    "\n",
    "1. Do not include data about the MLS or CFL in any of the work you are doing, we're only interested in the Big 4 in this assignment.\n",
    "2. I highly suggest that you first tackle the four correlation questions in order, as they are all similar and worth the majority of grades for this assignment. This is by design!\n",
    "3. It's fair game to talk with peers about high level strategy as well as the relationship between metropolitan areas and sports teams. However, do not post code solving aspects of the assignment (including such as dictionaries mapping areas to teams, or regexes which will clean up names).\n",
    "4. There may be more teams than the assert statements test, remember to collapse multiple teams in one city into a single value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "For this question, calculate the win/loss ratio's correlation with the population of the city it is in for the **NHL** using **2018** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "nhl_df=pd.read_csv(\"assets/nhl.csv\")\n",
    "cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "def clean_nhl_df():\n",
    "    # load data\n",
    "    nhl_df=pd.read_csv(\"assets/nhl.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "    # cleaning the cities dataframe\n",
    "    cities[\"NHL\"] = cities[\"NHL\"].apply(lambda x: re.sub(r\"\\[.+\\]\", \"\", x))\n",
    "    cities[\"NHL\"] = cities[\"NHL\"].replace({\"RangersIslandersDevils\": \"Rangers,Islanders,Devils\",\n",
    "                                           \"KingsDucks\": \"Kings,Ducks\",\n",
    "                                           \"Red Wings\": \"Red,Wings\", \n",
    "                                           \"Maple Leafs\": \"Maple,Leafs\", \n",
    "                                           \"Blue Jackets\": \"Blue,Jackets\",\n",
    "                                           \"Golden Knights\": \"Golden,Knights\" })\n",
    "    cities[\"NHL\"] = cities[\"NHL\"].apply(lambda x: x.split(\",\"))\n",
    "    cities = cities.explode(\"NHL\")\n",
    "\n",
    "    # cleaning the nhl_df dataframe\n",
    "    nhl_df = nhl_df[nhl_df[\"year\"] == 2018]\n",
    "    nhl_df[\"team\"] = nhl_df[\"team\"].apply(lambda x: x.replace(\"*\", \"\"))\n",
    "    nhl_df[\"team\"] = nhl_df[\"team\"].apply(lambda x: x.split(\" \")[-1])\n",
    "\n",
    "    # merge the dataframes\n",
    "    df = pd.merge(cities, nhl_df, left_on=\"NHL\", right_on=\"team\")\n",
    "    df = df[[\"Metropolitan area\", \"Population (2016 est.)[8]\", \"NHL\", \"team\", \"W\", \"L\"]]\n",
    "    df[\"W-L%\"] = df[\"W\"].astype(\"int\")/(df[\"W\"].astype(\"int\") + df[\"L\"].astype(\"int\"))\n",
    "    df[\"Population (2016 est.)[8]\"] = df[\"Population (2016 est.)[8]\"].astype(\"float\")\n",
    "    df[\"W-L%\"] = df[\"W-L%\"].astype(\"float\")\n",
    "\n",
    "    # drop duplicated columns\n",
    "    df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"].mean() # mean of NY W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"].mean() # mean of LA W-L%\n",
    "    df = df.drop_duplicates(subset=\"Metropolitan area\").reset_index()\n",
    "    df = df.drop(columns=\"index\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def nhl_correlation(): \n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    df = clean_nhl_df()\n",
    "\n",
    "    population_by_region = df[\"Population (2016 est.)[8]\"] # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = df[\"W-L%\"] # pass in win/loss ratio from nhl_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    assert len(population_by_region) == len(win_loss_by_region), \"Q1: Your lists must be the same length\"\n",
    "    assert len(population_by_region) == 28, \"Q1: There should be 28 teams being analysed for NHL\"\n",
    "    \n",
    "    result = stats.pearsonr(population_by_region, win_loss_by_region)\n",
    "    \n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "For this question, calculate the win/loss ratio's correlation with the population of the city it is in for the **NBA** using **2018** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df=pd.read_csv(\"assets/nba.csv\")\n",
    "cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "def clean_nba_df():\n",
    "    # load data\n",
    "    nba_df=pd.read_csv(\"assets/nba.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "    \n",
    "    # cleaning the cities dataframe\n",
    "    cities[\"NBA\"] = cities[\"NBA\"].apply(lambda x: re.sub(r\"\\[.+\\]\", \"\", x))\n",
    "    cities[\"NBA\"] = cities[\"NBA\"].replace({\"KnicksNets\": \"Knicks,Nets\",\n",
    "                                           \"LakersClippers\": \"Lakers,Clippers\",\n",
    "                                           \"Trail Blazers\": \"Trail,Blazers\"})\n",
    "    cities[\"NBA\"] = cities[\"NBA\"].apply(lambda x: x.split(\",\"))\n",
    "    cities = cities.explode(\"NBA\")\n",
    "\n",
    "    # cleaning the nhl_df dataframe\n",
    "    nba_df = nba_df[nba_df[\"year\"] == 2018]\n",
    "    nba_df[\"team\"] = nba_df[\"team\"].apply(lambda x: re.sub(r\"(\\*)*\\s\\(\\d+\\)\", \"\", x))\n",
    "    nba_df[\"team\"] = nba_df[\"team\"].apply(lambda x: x.split(\" \")[-1])\n",
    "\n",
    "    # merge the dataframes\n",
    "    df = pd.merge(cities, nba_df, left_on=\"NBA\", right_on=\"team\")\n",
    "    df = df.rename(columns={\"W/L%\": \"W-L%\"})\n",
    "    df = df[[\"Metropolitan area\", \"Population (2016 est.)[8]\", \"NBA\", \"team\", \"W\", \"L\", \"W-L%\"]]\n",
    "    df[\"Population (2016 est.)[8]\"] = df[\"Population (2016 est.)[8]\"].astype(\"float\")\n",
    "    df[\"W-L%\"] = df[\"W-L%\"].astype(\"float\")\n",
    "    \n",
    "    # drop duplicated columns\n",
    "    df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"].mean() # mean of NY W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"].mean() # mean of LA W-L%\n",
    "    df = df.drop_duplicates(subset=\"Metropolitan area\").reset_index()\n",
    "    df = df.drop(columns=\"index\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def nba_correlation():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    df = clean_nba_df()\n",
    "\n",
    "    population_by_region = df[\"Population (2016 est.)[8]\"] # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = df[\"W-L%\"] # pass in win/loss ratio from nhl_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    assert len(population_by_region) == len(win_loss_by_region), \"Q2: Your lists must be the same length\"\n",
    "    assert len(population_by_region) == 28, \"Q2: There should be 28 teams being analysed for NBA\"\n",
    "\n",
    "    result = stats.pearsonr(population_by_region, win_loss_by_region)\n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "For this question, calculate the win/loss ratio's correlation with the population of the city it is in for the **MLB** using **2018** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "mlb_df=pd.read_csv(\"assets/mlb.csv\")\n",
    "cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "def clean_mlb_df():\n",
    "    # load data\n",
    "    mlb_df=pd.read_csv(\"assets/mlb.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "    # cleaning the cities dataframe\n",
    "    cities[\"MLB\"] = cities[\"MLB\"].apply(lambda x: re.sub(r\"\\[.+\\]\", \"\", x))\n",
    "    cities[\"MLB\"] = cities[\"MLB\"].replace({\"Blue Jays\": \"Blue,Jays\", \n",
    "                                           \"CubsWhite Sox\": \"Cubs,White,Sox\", \n",
    "                                           \"DodgersAngels\": \"Dodgers,Angels\", \n",
    "                                           \"GiantsAthletics\": \"Giants,Athletics\", \n",
    "                                           \"YankeesMets\": \"Yankees,Mets\",\n",
    "                                           \"Red Sox\": \"Red,Sox\"})\n",
    "    cities[\"MLB\"] = cities[\"MLB\"].apply(lambda x: x.split(\",\"))\n",
    "    cities = cities.explode(\"MLB\")\n",
    "\n",
    "    # cleaning the nhl_df dataframe\n",
    "    mlb_df = mlb_df[mlb_df[\"year\"] == 2018]\n",
    "    mlb_df[\"team\"] = mlb_df[\"team\"].apply(lambda x: x.split(\" \")[-1])\n",
    "\n",
    "    # merge the dataframes\n",
    "    df = pd.merge(cities, mlb_df, left_on=\"MLB\", right_on=\"team\")\n",
    "    df = df[[\"Metropolitan area\", \"Population (2016 est.)[8]\", \"MLB\", \"team\", \"W\", \"L\", \"W-L%\"]]\n",
    "    df[\"Population (2016 est.)[8]\"] = df[\"Population (2016 est.)[8]\"].astype(\"float\")\n",
    "    df[\"W-L%\"] = df[\"W-L%\"].astype(\"float\")\n",
    "\n",
    "    # drop duplicated columns\n",
    "    df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"].mean() # mean of NY W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"].mean() # mean of LA W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"San Francisco Bay Area\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"San Francisco Bay Area\", \"W-L%\"].maen() # mean of SF W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"Chicago\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"Chicago\", \"W-L%\"].mean() # mean of CH W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"Boston\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"Boston\", \"W-L%\"].mean() # mean of BO W-L%\n",
    "    df = df.drop_duplicates(subset=\"Metropolitan area\").reset_index()\n",
    "    df = df.drop(columns=\"index\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mlb_correlation(): \n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    df = clean_mlb_df()\n",
    "\n",
    "    population_by_region = df[\"Population (2016 est.)[8]\"] # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = df[\"W-L%\"] # pass in win/loss ratio from mlb_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    assert len(population_by_region) == len(win_loss_by_region), \"Q3: Your lists must be the same length\"\n",
    "    assert len(population_by_region) == 26, \"Q3: There should be 26 teams being analysed for MLB\"\n",
    "\n",
    "    result = stats.pearsonr(population_by_region, win_loss_by_region)\n",
    "\n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "For this question, calculate the win/loss ratio's correlation with the population of the city it is in for the **NFL** using **2018** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "nfl_df=pd.read_csv(\"assets/nfl.csv\")\n",
    "cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "def clean_nfl_df():\n",
    "    # load data\n",
    "    nfl_df=pd.read_csv(\"assets/nfl.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "    # cleaning the cities dataframe\n",
    "    cities[\"NFL\"] = cities[\"NFL\"].apply(lambda x: re.sub(r\"\\[.+\\]\", \"\", x))\n",
    "    cities[\"NFL\"] = cities[\"NFL\"].replace({\"GiantsJets\": \"Giants,Jets\",\n",
    "                                           \"RamsChargers\": \"Rams,Chargers\",\n",
    "                                           \"49ersRaiders\": \"49ers,Raiders\"\n",
    "                                           })\n",
    "    cities[\"NFL\"] = cities[\"NFL\"].apply(lambda x: x.split(\",\"))\n",
    "    cities = cities.explode(\"NFL\")\n",
    "\n",
    "    # cleaning the nhl_df dataframe\n",
    "    nfl_df = nfl_df[nfl_df[\"year\"] == 2018]\n",
    "    nfl_df[\"team\"] = nfl_df[\"team\"].apply(lambda x: re.sub(r\"(\\*|\\+)\", \"\", x))\n",
    "    nfl_df[\"team\"] = nfl_df[\"team\"].apply(lambda x: x.split(\" \")[-1])\n",
    "\n",
    "    # merge the dataframes\n",
    "    df = pd.merge(cities, nfl_df, left_on=\"NFL\", right_on=\"team\")\n",
    "    df = df[[\"Metropolitan area\", \"Population (2016 est.)[8]\", \"NFL\", \"team\", \"W\", \"L\", \"W-L%\"]]\n",
    "    df[\"Population (2016 est.)[8]\"] = df[\"Population (2016 est.)[8]\"].astype(\"float\")\n",
    "    df[\"W-L%\"] = df[\"W-L%\"].astype(\"float\")\n",
    "\n",
    "    # drop duplicated columns\n",
    "    df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"New York City\", \"W-L%\"].mean() # mean of NY W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"Los Angeles\", \"W-L%\"].mean() # mean of LA W-L%\n",
    "    df.loc[df[\"Metropolitan area\"] == \"San Francisco Bay Area\", \"W-L%\"] = df.loc[df[\"Metropolitan area\"] == \"San Francisco Bay Area\", \"W-L%\"].mean() # mean of SF W-L%\n",
    "    df = df.drop_duplicates(subset=\"Metropolitan area\").reset_index()\n",
    "    df = df.drop(columns=\"index\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def nfl_correlation(): \n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    df = clean_nfl_df()\n",
    "    \n",
    "    population_by_region = df[\"Population (2016 est.)[8]\"] # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = df[\"W-L%\"] # pass in win/loss ratio from nfl_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    assert len(population_by_region) == len(win_loss_by_region), \"Q4: Your lists must be the same length\"\n",
    "    assert len(population_by_region) == 29, \"Q4: There should be 29 teams being analysed for NFL\"\n",
    "\n",
    "    result = stats.pearsonr(population_by_region, win_loss_by_region)\n",
    "\n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "In this question I would like you to explore the hypothesis that **given that an area has two sports teams in different sports, those teams will perform the same within their respective sports**. How I would like to see this explored is with a series of paired t-tests (so use [`ttest_rel`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html)) between all pairs of sports. Are there any sports where we can reject the null hypothesis? Again, average values where a sport has multiple teams in one region. Remember, you will only be including, for each sport, cities which have teams engaged in that sport, drop others as appropriate. This question is worth 20% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "mlb_df=pd.read_csv(\"assets/mlb.csv\")\n",
    "nhl_df=pd.read_csv(\"assets/nhl.csv\")\n",
    "nba_df=pd.read_csv(\"assets/nba.csv\")\n",
    "nfl_df=pd.read_csv(\"assets/nfl.csv\")\n",
    "cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "# clean the dataframes\n",
    "def clean_dfs():\n",
    "    nfl_df = clean_nfl_df()\n",
    "    nba_df = clean_nba_df()\n",
    "    nhl_df = clean_nhl_df()   \n",
    "    mlb_df = clean_mlb_df()\n",
    "    \n",
    "    \n",
    "    nfl_df = nfl_df[[\"Metropolitan area\", \"W-L%\"]]\n",
    "    nba_df = nba_df[[\"Metropolitan area\", \"W-L%\"]]\n",
    "    nhl_df = nhl_df[[\"Metropolitan area\", \"W-L%\"]]  \n",
    "    mlb_df = mlb_df[[\"Metropolitan area\", \"W-L%\"]]\n",
    "    \n",
    "    return (nfl_df, nba_df, nhl_df, mlb_df)\n",
    "\n",
    "def calculate_p_values(leagues):\n",
    "    p_values = pd.DataFrame(columns=leagues.keys(), index=leagues.keys())\n",
    "    for (league_name1, league_df1) in leagues.items():\n",
    "        for (league_name2, league_df2) in leagues.items():\n",
    "            merged_league = pd.merge(league_df1, league_df2, on=\"Metropolitan area\")\n",
    "            p_values.loc[league_name1, league_name2] = stats.ttest_rel(merged_league[\"W-L%_x\"], merged_league[\"W-L%_y\"])[1]\n",
    "            \n",
    "    return p_values\n",
    "\n",
    "def sports_team_performance():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    (nfl_df, nba_df, nhl_df, mlb_df) = clean_dfs()\n",
    "    \n",
    "    leagues = {\"NFL\": nfl_df, \"NBA\": nba_df, \"NHL\": nhl_df, \"MLB\": mlb_df}\n",
    "    p_values_dict = calculate_p_values(leagues)\n",
    "    p_values = pd.DataFrame(p_values_dict).astype(\"float\")\n",
    "    \n",
    "#     # Note: p_values is a full dataframe, so df.loc[\"NFL\",\"NBA\"] should be the same as df.loc[\"NBA\",\"NFL\"] and\n",
    "#     # df.loc[\"NFL\",\"NFL\"] should return np.nan\n",
    "#     sports = ['NFL', 'NBA', 'NHL', 'MLB']\n",
    "#     p_values = pd.DataFrame({k:np.nan for k in sports}, index=sports)\n",
    "    \n",
    "    assert abs(p_values.loc[\"NBA\", \"NHL\"] - 0.02) <= 1e-2, \"The NBA-NHL p-value should be around 0.02\"\n",
    "    assert abs(p_values.loc[\"MLB\", \"NFL\"] - 0.80) <= 1e-2, \"The MLB-NFL p-value should be around 0.80\"\n",
    "    return p_values"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mooc_adswpy_1_v2_assignment1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
